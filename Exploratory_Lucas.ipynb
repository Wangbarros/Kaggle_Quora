{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "import statsmodels.api as sm\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#nltk.download() #<---Use this command to download WordNet in corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/media/fexunexa/Tree/Kaggle/Git Quora/data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cstahl12/titanic/titanic-with-keras\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "\n",
    "print('Keras using {} backend'.format(keras.backend.backend()))  #https://keras.io/backend/\n",
    "\n",
    "batch_size = np.round(len(quora_train_features_tf)*0.10)\n",
    "num_classes = 2\n",
    "epochs = 50\n",
    "\n",
    "quora_train_features_tf_reshape = quora_train_features_tf.astype('float32')\n",
    "quora_test_features_tf_reshape = quora_test_features_tf.astype('float32')\n",
    "\n",
    "print(quora_train_features_tf_reshape.shape[0], 'train samples')\n",
    "print(quora_test_features_tf_reshape.shape[0], 'test samples')\n",
    "\n",
    "quora_train_y_tf_cat = np_utils.to_categorical(quora_train_y_tf, 2)\n",
    "quora_test_y_tf_cat = np_utils.to_categorical(quora_test_y_tf, 2)\n",
    "\n",
    "# convert class vectors to binary class matrices for categorical cross_entropy\n",
    "#y_train_kr = to_categorical(y_traincv)\n",
    "#y_test_kr = to_categorical(y_testcv)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=quora_train_features_tf_reshape.shape[1], activation='relu', units=200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(input_dim=200, activation='relu', units=200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(input_dim=200, activation='relu', units=24)) #activation='softmax'\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(input_dim=24, kernel_initializer='uniform', activation='sigmoid', units=1)) #units=2 for cat.cr.ent\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #loss='categorical_crossentropy',\n",
    "              #loss='mean_squared_error',\n",
    "              optimizer=RMSprop(), \n",
    "              #optimizer=SGD(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(quora_train_features_tf_reshape, quora_train_y_tf_cat,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(quora_test_features_tf_reshape, quora_test_y_tf_cat))\n",
    "\n",
    "score = model.evaluate(quora_test_features_tf_reshape, quora_test_y_tf_cat, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "predict_tf = model.predict_classes(quora_test_features_tf_reshape) #y_pred = np.around(model.predict(x_test_kr)[:,1])\n",
    "print(log_loss(quora_test_y_tf_cat,predict_tf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Vendo as 3 primeiras informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Selecionando linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train[4:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Selecionando Coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train['question1'][200:205]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Selecionar duas colunas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.loc[1:5,['question1','question2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Selecionando pela posição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.iloc[1:5,2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Selecionando informação específica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train[train.is_duplicate == 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Verificando por vazios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vazioQ1 = []\n",
    "vazioQ2 = []\n",
    "\n",
    "for i in range(0,len(train.question2)):\n",
    "    if type(train.question2[i]) is float:\n",
    "        vazioQ2.append(i)\n",
    "    if type(train.question1[i]) is float:\n",
    "        vazioQ1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(vazioQ1, vazioQ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Lower Case and removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lower_rm_punct(phrase):\n",
    "    if type(phrase) is not float:\n",
    "        phrase = phrase.lower()\n",
    "        phrase = phrase.translate(str.maketrans('','', string.punctuation))\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.question1 = train.question1.apply(lambda x: lower_rm_punct(x))\n",
    "train.question2 = train.question2.apply(lambda x: lower_rm_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Function to Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rm_stopwords(phrase):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    if type(phrase) is not float:\n",
    "        phrase = [i for i in phrase.split() if i not in stop]\n",
    "        phrase = ' '.join(phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_stopw = train\n",
    "\n",
    "train_stopw.question1 = train.question1.apply(lambda x: rm_stopwords(x))\n",
    "train_stopw.question2 = train.question2.apply(lambda x: rm_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Function to \"stem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### From Stackoverflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### At the very basics of it, the major difference between the porter and lancaster stemming algorithms is that the lancaster stemmer is significantly more aggressive than the porter stemmer. The three major stemming algorithms in use today are Porter, Snowball(Porter2), and Lancaster (Paice-Husk), with the aggressiveness continuum basically following along those same lines. Porter is the least aggressive algorithm, with the specifics of each algorithm actually being fairly lengthy and technical. Here is a break down for you though:\n",
    "\n",
    "##### Porter: Most commonly used stemmer without a doubt, also one of the most gentle stemmers. One of the few stemmers that actually has Java support which is a plus, though it is also the most computationally intensive of the algorithms(Granted not by a very significant margin). It is also the oldest stemming algorithm by a large margin.\n",
    "\n",
    "##### Porter2: Nearly universally regarded as an improvement over porter, and for good reason. Porter himself in fact admits that it is better than his original algorithm. Slightly faster computation time than porter, with a fairly large community around it.\n",
    "\n",
    "##### Lancaster: Very aggressive stemming algorithm, sometimes to a fault. With porter and snowball, the stemmed representations are usually fairly intuitive to a reader, not so with Lancaster, as many shorter words will become totally obfuscated. The fastest algorithm here, and will reduce your working set of words hugely, but if you want more distinction, not the tool you would want.\n",
    "\n",
    "##### Honestly, I feel that Snowball is usually the way to go. There are certain circumstances in which Lancaster will hugely trim down your working set, which can be very useful, however the marginal speed increase over snowball in my opinion is not worth the lack of precision. Porter has the most implementations though and so is usually the default go-to algorithm, but if you can, use snowball.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stem_lancaster(phrase):\n",
    "    LS = LancasterStemmer()\n",
    "    \n",
    "    if type(phrase) is not float:\n",
    "        phrase = [LS.stem(i) for i in phrase.split()]\n",
    "        phrase = ' '.join(phrase)\n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Function to \"lemmatize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lemm_wordnet(phrase):\n",
    "    lemm = WordNetLemmatizer()\n",
    "\n",
    "    if type(phrase) is not float:\n",
    "        phrase = [lemm.lemmatize(i) for i in phrase.split()]\n",
    "        phrase = ' '.join(phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Some Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def freq_all_text(question1,question2): #Pass all question\n",
    "\n",
    "    question1_freq = []\n",
    "    question2_freq = []\n",
    "    for i in range(0,len(question1)):\n",
    "        question1_freq += question1[i].split()\n",
    "        question2_freq += str(question2[i]).split()\n",
    "    \n",
    "    len_question_freq = len(set(question1_freq + question2_freq))\n",
    "    \n",
    "    question1_freq = FreqDist(question1_freq)\n",
    "    question2_freq = FreqDist(question2_freq)\n",
    "\n",
    "    return question1_freq, question2_freq, len_question_freq\n",
    "\n",
    "def percentage_by_text(phrase1, phrase2, question1_freq, question2_freq, len_question_freq):\n",
    "    phrase2 = str(phrase2)\n",
    "    \n",
    "    for i in range(0, len(phrase1)):\n",
    "            less_common_phrase1 = [i for i in set(phrase1.split()) if i not in phrase2.split()]\n",
    "            less_common_phrase2 = [i for i in set(phrase2.split()) if i not in phrase1.split()]\n",
    "            \n",
    "    freq_less_common_phrase1 = 0\n",
    "    freq_less_common_phrase2 = 0\n",
    "            \n",
    "    for i in less_common_phrase1:\n",
    "        freq_less_common_phrase1 += question1_freq[i]\n",
    "    \n",
    "    freq_less_common_phrase1 = freq_less_common_phrase1/len_question_freq\n",
    "    \n",
    "    for i in less_common_phrase2:\n",
    "        freq_less_common_phrase2 += question2_freq[i]\n",
    "    \n",
    "    freq_less_common_phrase2 = freq_less_common_phrase2/len_question_freq\n",
    "    \n",
    "    if freq_less_common_phrase1 == 0:\n",
    "        return freq_less_common_phrase2\n",
    "    \n",
    "    elif freq_less_common_phrase2 == 0:\n",
    "        return freq_less_common_phrase1\n",
    "    \n",
    "    elif freq_less_common_phrase1 >= freq_less_common_phrase2:\n",
    "        return freq_less_common_phrase2\n",
    "    \n",
    "    else:\n",
    "        return freq_less_common_phrase1\n",
    "\n",
    "def percentages_by_phrase(phrase1,phrase2):\n",
    "    phrase2 = str(phrase2)\n",
    "    \n",
    "    #Percentage between numbers of words\n",
    "    if len(phrase1.split()) >= len(phrase2.split()):\n",
    "        dif_wdsize = len(phrase2.split())/len(phrase1.split())\n",
    "    else:\n",
    "        dif_wdsize = len(phrase1.split())/len(phrase2.split())\n",
    "   \n",
    "    #Percentage between numbers of letters\n",
    "    if len(phrase1) >= len(phrase2):\n",
    "        dif_ltsize = len(phrase2)/len(phrase1)\n",
    "    else:\n",
    "        dif_ltsize = len(phrase1)/len(phrase2)\n",
    "        \n",
    "    #Percentage between letters and words per phrase\n",
    "    dif_ltwdsize = (len(phrase1.split())/len(phrase1))/(len(phrase2.split())/len(phrase2))\n",
    "\n",
    "    #Percentage of equal words\n",
    "    equal = [i for i in set(phrase1.split()) if i in set(phrase2.split())]\n",
    "    equal = ' '.join(equal)\n",
    "    \n",
    "    unique = set((phrase1 + ' ' + phrase2).split()) #Counting all unique words between the 2 phrases\n",
    "    unique = ' '.join(unique)\n",
    "    \n",
    "    perc_equal_w = len(equal)/len(unique)\n",
    "    \n",
    "    return dif_wdsize, dif_ltsize, dif_ltwdsize, perc_equal_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = train[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Saving some new informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(train.question1)):\n",
    "    p_size_word, p_size_letters, p_word_letters, p_equal_w = percentages_by_phrase(train.loc[i,'question1'],train.loc[i,'question2'])\n",
    "    \n",
    "    train.loc[i, 'p_size_word'] = p_size_word\n",
    "    train.loc[i, 'p_size_letters'] = p_size_letters\n",
    "    train.loc[i, 'p_word_letters'] = p_word_letters\n",
    "    train.loc[i, 'p_equal_w'] = p_equal_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "question1_freq, question2_freq, len_question_freq = freq_all_text(train.question1,train.question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(train.question1)):\n",
    "    \n",
    "    p_less_common = percentage_by_text(train.loc[i,'question1'],train.loc[i,'question2'], question1_freq, question2_freq, len_question_freq)\n",
    "    \n",
    "    train.loc[i, 'p_less_common'] = p_less_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "logit = sm.Logit(train['is_duplicate'], train.loc[:,['p_less_common','p_equal_w','p_word_letters']])\n",
    "\n",
    "# fit the model\n",
    "result = logit.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result.predict(train.loc[:,['p_less_common','p_equal_w','p_word_letters']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
